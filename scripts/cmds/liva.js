const axios = require('axios');

module.exports = {
  config: {
    name: "liva",
    author: "Bruno",
    version: "1.0.0",
    countDown: 5,
    role: 0,
    category: "Ai",
    shortDescription: {
      en: "Automatic Response Bot"
    }
  },
  onStart: async function ({ api }) {
    // Initialisation si nÃ©cessaire
  },
  onChat: async function ({ event, api }) {
    const message = event.body.toLowerCase();

    // Titre Ã  ajouter Ã  chaque rÃ©ponse
    const title = "ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—§ ğ—”ğ—œ\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\n";

    // RÃ©ponse automatique pour les messages spÃ©cifiques
    if (message.includes("bonjour") || message.includes("hello")) {
      api.sendMessage(`${title}Bonjour! Comment puis-je vous aider aujourd'hui?`, event.threadID);
      return;
    }

    // Logique pour traiter d'autres messages avec l'API
    try {
      const prompt = encodeURIComponent(message);
      const apiUrl = `https://discussion-continue-gem29.vercel.app/api?ask=${prompt}`;

      const response = await axios.get(apiUrl);

      if (response.data && response.data.response) {
        api.sendMessage(`${title}${response.data.response}`, event.threadID);
      } else {
        api.sendMessage(`${title}Je n'ai pas pu obtenir de rÃ©ponse Ã  votre demande.`, event.threadID);
      }
    } catch (error) {
      console.error('Error making Llama API request:', error.message);
      api.sendMessage(`${title}Une erreur est survenue lors du traitement de votre demande.`, event.threadID);
    }
  }
};
